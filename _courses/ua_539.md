---
layout: page
title: LING 539
description: Statistical Natural Language Processing
img: assets/img/539.png
importance: 1
category: graduate
related_publications: false
---

This 3-credit course introduces the key concepts underlying statistical natural language processing, a domain which includes "traditional" statistical approaches like Bayesian classifiers as well as more modern neural-based approaches whose design is nonetheless statistically-based. Students will learn a variety of techniques for the computational modeling of natural language, including: n-gram models, smoothing, Hidden Markov models, Bayesian inference, expectation maximization, the Viterbi algorithm, the Inside-Outside algorithm for probabilistic context-free grammars, and higher-order language models. This course complements the introductory course in symbolic and analytic computational approaches to language, [LING/CSC/PSY 538 _Computational Linguistics_](/courses/ua_538).

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/539.png" title="sample slide" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Given the performance improvements (_and massive hype!_) around [transformer](https://jalammar.github.io/illustrated-transformer/)-based LLMs since 2022, many students are eager to learn how to work with transformer-based LLMs like OpenAI's GPT models, Google's Gemini, Anthropic's Claude, and many others. However, rather than rush into what it takes simply to _use_ such models, our graduate program gives students a solid foundation in statistical approaches in order to understand more deeply what problems they're designed to solve, how they address those problems, and what their limitations are. This is the first course in a two-course series (LING 539 and LING 582) which brings students from a basic introduction to statistical approaches up to working with LLMs. In this first course, students come to understand a range of foundational natural language processing (NLP) topics, such as the special properties of language as data, text preprocessing and why it is needed, and the importance of choosing effective representations for language-based objects. We introduce basic concepts in machine learning and text classification algorithms as students learn the concepts underlying classification with naive Bayes and logistic regression. Students learn the utility of a variety of word representations (up to static embeddings), then apply these principles to sequence labeling (part of speech tagging, shallow parsing/chunking, etc.), and structured prediction (chart-based parsing, transition-based dependency parsing). Students will practice applying new concepts (including programming) first in low-stakes [TopHat](https://tophat.com/) review activities, and will then demonstrate their ability to use these concepts in a series of realistic programming assignments. Students enrolled in graduate-level sections will also participate in a private class competition on Kaggle, where they can apply an approach of their choice to an open-ended text classification task.

## Most recent syllabus

<div class="row justify-content-sm-center">
  <div class="col-sm mt-3 mt-md-0">
    <object data="/assets/pdf/LING539-2026Spr-Jackson.pdf" type='application/pdf' width="100%" height="800">
    </object>
  </div>
</div>

### All past syllabi for this course

- _Spring 2026 7W2_ (online)(in prep)
- [Spring 2026 15W](/assets/pdf/LING539-2026Spr-Jackson.pdf) (in person)
- [Spring 2024](/assets/pdf/LING539-2024Spr-Jackson.pdf) (in person)
- [Spring 2023](/assets/pdf/LING539-2023Spr-Jackson.pdf) (online)
